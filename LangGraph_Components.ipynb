{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8920a97a-84ad-4fb0-b3d2-425a6789b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today? ðŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Environment and Ollama setup\n",
    "# This cell loads environment variables, sets up the Ollama model and API URL, and prints them for verification.\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables (optional)\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set Ollama model and API URL\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"gemma3:1b\")\n",
    "OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434\")\n",
    "\n",
    "def get_completion_from_messages(messages, model=OLLAMA_MODEL, temperature=0):\n",
    "    \"\"\"\n",
    "    Generates responses from Ollama LLM using a list of messages (for chat-style prompts)\n",
    "    \"\"\"\n",
    "    # Concatenate all messages into a single prompt string\n",
    "    prompt = \"\\n\".join([msg[\"content\"] for msg in messages])\n",
    "    url = f\"{OLLAMA_API_URL}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama API error : {response.status_code} {response.text}\")\n",
    "    data = response.json()\n",
    "    return data['response']\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello world\"}\n",
    "]\n",
    "\n",
    "response = get_completion_from_messages(messages, model=\"gemma3:1b\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70156ec1-a447-4ce3-b925-0f5723034d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d6953e-74b6-4c9d-a7b7-a02fbaa41fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e3fc96-a6ba-437a-8d0c-b04c50f57d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
      "tavily_search_results_json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11752\\144666821.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=4) #increased number of results\n"
     ]
    }
   ],
   "source": [
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d545c062-2fd3-4cf1-bad0-76b4514cfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17467d37-04dc-4297-839c-23269fdcf69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        self.model = model\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_ai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_ai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e195d6-b36a-4b7e-a3d0-46d467aa40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper or use your function directly\n",
    "def ollama_chat(messages, model=\"gemma3:1b\", temperature=0):\n",
    "    return get_completion_from_messages(messages, model=model, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e094f01c-6a2b-4f4e-95ba-59779bbfa3b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mprompt\u001b[49m}]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d54dd-cc34-4787-9120-153b44c435d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ollama_chat(messages)  #reduce inference cost\n",
    "abot = Agent(ollama_chat, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56974183-2d77-4af2-ab2c-1c61296b67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaModelWrapper:\n",
    "    def __init__(self, model_name=\"gemma3:1b\", temperature=0):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def invoke(self, messages):\n",
    "        # messages should be a list of LangChain message objects or dicts\n",
    "        # Convert LangChain messages to dicts if needed\n",
    "        prompt_msgs = []\n",
    "        for msg in messages:\n",
    "            if hasattr(msg, \"content\"):\n",
    "                prompt_msgs.append({\"role\": \"user\", \"content\": msg.content})\n",
    "            elif isinstance(msg, dict):\n",
    "                prompt_msgs.append(msg)\n",
    "        return get_completion_from_messages(prompt_msgs, model=self.model_name, temperature=self.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cdc93-d641-47d3-85ac-fd4025217848",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaModelWrapper(model_name=\"gemma3:1b\")\n",
    "abot = Agent(ollama_model, [tool], system=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0392a8c-a9bd-48f0-8e13-3b06dcaebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
